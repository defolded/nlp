{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71e7fb7-aacf-4b6e-8494-64a668add5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             example        label\n",
      "0  Seller: Hello Buyer: Wow love the Couch. SO lo...    furniture\n",
      "1  Seller: I am selling this for $28500. Buyer: I...          car\n",
      "2  Buyer: I'd like to negotiate a lower price for...      housing\n",
      "3  Seller: Hi!  Are you interested in my headphon...  electronics\n",
      "4  Seller: Hi. Were you interested in the mirror?...    furniture\n",
      "                                             example      label\n",
      "0  Seller: Hi are you interested in buying my Pin...  furniture\n",
      "1  Buyer: Hello I am interested in your property ...    housing\n",
      "2  Buyer: Hello . How long have you owned the dre...  furniture\n",
      "3  Buyer: I am very interested place you have for...    housing\n",
      "4  Buyer: Hey, nice car you have here, how long h...        car\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('seed.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Check the first few rows of the data\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2cb622-ff62-4de1-9c60-57fe4f625da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_data(data):\n",
    "    return tokenizer(data['example'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "train_inputs = tokenize_data(train_data)\n",
    "test_inputs = tokenize_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94183307-771e-4c15-a2ee-ac4b13ad083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels = label_encoder.fit_transform(train_data['label'])\n",
    "test_labels = label_encoder.transform(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05db66b1-b263-462d-9022-de032348b510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# ...\n",
    "\n",
    "# Create Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26369ef7-f6e1-4cd2-8e6b-182d1f9037c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NIKITA\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):  # You may need to adjust the number of epochs\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b03594-b7d9-4be3-baf6-ca4e5092bed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.827\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_inputs = {key: val.to(device) for key, val in test_inputs.items()}\n",
    "    outputs = model(**test_inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Evaluate accuracy or other metrics\n",
    "accuracy = (predictions == test_labels).mean()\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7fd8e1b-23d0-45b8-8edf-da40b9acaca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Accuracy = 30.00%\n",
      "Iteration 2: Accuracy = 57.00%\n",
      "Iteration 3: Accuracy = 91.00%\n",
      "Iteration 4: Accuracy = 98.00%\n",
      "Iteration 5: Accuracy = 99.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv('seed.csv')\n",
    "train_inputs = tokenize_data(train_data)\n",
    "train_labels = label_encoder.transform(train_data['label'])\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "train_dataset = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "model.to(device)\n",
    "\n",
    "# Training hyperparameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# Active learning loop\n",
    "num_iterations = 5  # You can adjust the number of active learning iterations\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate accuracy on the entire training set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_inputs = {key: val.to(device) for key, val in train_inputs.items()}\n",
    "        outputs = model(**train_inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions = (predictions == train_labels).sum().item()\n",
    "        accuracy = correct_predictions / len(train_labels)\n",
    "\n",
    "    print(f\"Iteration {iteration + 1}: Accuracy = {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Calculate entropy on the entire training set\n",
    "    with torch.no_grad():\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        entropy = -torch.sum(probabilities * torch.log2(probabilities + 1e-10), dim=1)\n",
    "\n",
    "    # Select the top k examples with highest entropy for retraining\n",
    "    k = int(len(train_dataset) * 0.1)  # You can adjust the fraction of examples to retrain\n",
    "    uncertain_indices = torch.argsort(entropy, descending=True)[:k]\n",
    "\n",
    "    # Retrain the model on the selected uncertain examples\n",
    "    uncertain_inputs = train_inputs['input_ids'][uncertain_indices]\n",
    "    uncertain_attention_mask = train_inputs['attention_mask'][uncertain_indices]\n",
    "    uncertain_labels = train_labels[uncertain_indices]\n",
    "    uncertain_dataset = TensorDataset(uncertain_inputs, uncertain_attention_mask, uncertain_labels)\n",
    "    \n",
    "    train_loader = DataLoader(uncertain_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, attention_mask, labels = batch\n",
    "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8abebfb7-0700-4698-afa1-e11f91e5ebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Example   True Label  \\\n",
      "0    Seller: Hi are you interested in buying my Pin...    furniture   \n",
      "1    Buyer: Hello I am interested in your property ...      housing   \n",
      "2    Buyer: Hello . How long have you owned the dre...    furniture   \n",
      "3    Buyer: I am very interested place you have for...      housing   \n",
      "4    Buyer: Hey, nice car you have here, how long h...          car   \n",
      "..                                                 ...          ...   \n",
      "995  Buyer: Hi! Seller: Hello.  How are you? Buyer:...    furniture   \n",
      "996  Seller: Hi how are you? Buyer: I'm wonderful! ...  electronics   \n",
      "997  Buyer: hello I am interested in the yukon you ...          car   \n",
      "998  Seller: Hi there, are you interested in my pro...      housing   \n",
      "999  Seller: Hello. Buyer: Hi. Would you do 10$ on ...         bike   \n",
      "\n",
      "    Predicted Label  \n",
      "0         furniture  \n",
      "1           housing  \n",
      "2         furniture  \n",
      "3           housing  \n",
      "4               car  \n",
      "..              ...  \n",
      "995       furniture  \n",
      "996     electronics  \n",
      "997             car  \n",
      "998         housing  \n",
      "999            bike  \n",
      "\n",
      "[1000 rows x 3 columns]\n",
      "\n",
      "Precision on Test Set: 0.8849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Load the test dataset (assuming it's in a pandas DataFrame)\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Tokenize and encode the test dataset\n",
    "test_inputs = tokenizer(test_data['example'].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "test_labels = label_encoder.transform(test_data['label'])\n",
    "\n",
    "# Move the test dataset to the GPU if available\n",
    "test_inputs = {key: val.to(device) for key, val in test_inputs.items()}\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "\n",
    "# Run the model on the test dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "# Decode the predictions using the label encoder\n",
    "decoded_predictions = label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "\n",
    "# Calculate precision score\n",
    "precision = precision_score(test_labels.cpu().numpy(), predictions.cpu().numpy(), average='weighted')\n",
    "\n",
    "# Print the classification results\n",
    "results = pd.DataFrame({'Example': test_data['example'], 'True Label': test_data['label'], 'Predicted Label': decoded_predictions})\n",
    "print(results)\n",
    "\n",
    "# Print precision score\n",
    "print(f\"\\nPrecision on Test Set: {precision:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
